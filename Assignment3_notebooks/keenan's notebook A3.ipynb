{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e56f7b-4cf4-4636-b2a5-6072688f600a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764012989340}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"nyc_taxi.idk.yellow_trips_csv_v\")\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7a0dab-8c8d-4e28-9539-57e0bc12f562",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764022958661}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import radians, sin, cos, atan2, sqrt\n",
    "R=3959 #radius of earth in miles\n",
    "df_clean=(\n",
    "    df\n",
    "    .filter((df.fare_amount > 0) & (df.fare_amount<500) & (df.trip_distance >0))\n",
    "    .withColumn(\"pickup_hour\", F.hour(df.tpep_pickup_datetime))\n",
    "    .withColumn(\"pickup_dow\", F.dayofweek(df.tpep_pickup_datetime))\n",
    "    .withColumn(\"pickup_ts\",  F.unix_timestamp(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"dropoff_ts\", F.unix_timestamp(\"tpep_dropoff_datetime\"))\n",
    "    .withColumn(\"trip_duration_min\", (F.col(\"dropoff_ts\") - F.col(\"pickup_ts\")) / 60.0)\n",
    "    \n",
    "    \n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"pick_lat\", radians(df_clean.pickup_latitude))\n",
    "    .withColumn(\"pick_long\", radians(df_clean.pickup_longitude))\n",
    "    .withColumn(\"drop_lat\", radians(df_clean.dropoff_latitude))\n",
    "    .withColumn(\"drop_long\", radians(df_clean.dropoff_longitude))\n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"lat_diff\", df_clean.pick_lat - df_clean.drop_lat)\n",
    "    .withColumn(\"long_diff\", df_clean.pick_long - df_clean.drop_long)\n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"a\", sin(df_clean.lat_diff/2)**2 + cos(df_clean.pick_lat)*cos(df_clean.drop_lat)*sin(df_clean.long_diff/2)**2)\n",
    "    .withColumn(\"c\", 2*atan2(sqrt(F.col(\"a\")), sqrt(1-F.col(\"a\"))))\n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"straight_line_distance\", df_clean.c*R)\n",
    "\n",
    ")\n",
    "df_clean=df_clean.drop(\"pick_lat\", \"pick_long\", \"drop_lat\", \"drop_long\", \"lat_diff\", \"long_diff\", \"a\", \"c\")\n",
    "\n",
    "df_clean=df_clean.withColumn(\"high_fare\", F.when(df_clean.fare_amount >20,1).otherwise(0))\n",
    "numericFeatures = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_duration_min\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dow\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"improvement_surcharge\"\n",
    "]\n",
    "\n",
    "categoricalFeatures = [\n",
    "    \"VendorID\",\n",
    "    \"RateCodeID\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"payment_type\"\n",
    "]\n",
    "\n",
    "#remove nulls for cols used as features + label\n",
    "colsToKeepNotNull = numericFeatures + categoricalFeatures + [\"fare_amount\"]\n",
    "\n",
    "df_clean = df_clean.na.drop(subset=colsToKeepNotNull)\n",
    "train_df, test_df = df_clean.randomSplit([0.7,0.3], seed=42)\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31384d9-ae61-4e85-8731-bd1873415ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#index categoricals\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=c + \"_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for c in categoricalFeatures\n",
    "]\n",
    "\n",
    "#one-hot encode\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[c + \"_idx\" for c in categoricalFeatures],\n",
    "    outputCols=[c + \"_oh\" for c in categoricalFeatures]\n",
    ")\n",
    "\n",
    "#assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numericFeatures + [c + \"_oh\" for c in categoricalFeatures],\n",
    "    outputCol=\"features_unscaled\"\n",
    ")\n",
    "\n",
    "#scale for linear regression\n",
    "#withMean=False keeps vector sparse to avoid huge memory usage\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=False, #True\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "#linear regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "#full linear regression pipeline\n",
    "lrPipeline = Pipeline(\n",
    "    stages=indexers + [encoder, assembler, scaler, lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c0786d-750d-49a4-9842-4cb6ee2b5c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#param grid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.0, 0.1]) # [0.0, 0.01, 0.1]\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5]) # [0.0, 0.5, 1.0]\n",
    "    .addGrid(lr.maxIter, [50]) # [50, 100]\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluatorRmse = RegressionEvaluator(\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "cvLr = CrossValidator(\n",
    "    estimator=lrPipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluatorRmse,\n",
    "    numFolds=2,     #lower if needed\n",
    "    parallelism=1,  #lower if needed\n",
    "    collectSubModels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60e4bbc-e4b5-4204-bce8-86da2c004977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/ml_storage/ml_schema/ml_volume/sparkml_tmp\"\n",
    "cvLrModel = cvLr.fit(train_df)\n",
    "\n",
    "bestLrPipelineModel = cvLrModel.bestModel\n",
    "bestLrModel = bestLrPipelineModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1e4b60-fab0-4fdc-9245-fdebd4db5a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "keenan's notebook A3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
