{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e56f7b-4cf4-4636-b2a5-6072688f600a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764012989340}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"nyc_taxi.idk.yellow_trips_csv_v\")\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7a0dab-8c8d-4e28-9539-57e0bc12f562",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764022958661}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import radians, sin, cos, atan2, sqrt\n",
    "R=3959 #radius of earth in miles\n",
    "df_clean=(\n",
    "    df\n",
    "    .filter((df.fare_amount > 0) & (df.fare_amount<500))\n",
    "    .filter((F.col(\"trip_distance\") > 0) & (F.col(\"trip_distance\") < 100)) #filter out outlier trips\n",
    "    .withColumn(\"pickup_hour\", F.hour(df.tpep_pickup_datetime))\n",
    "    .withColumn(\"pickup_dow\", F.dayofweek(df.tpep_pickup_datetime))\n",
    "    .withColumn(\"pickup_ts\",  F.unix_timestamp(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"dropoff_ts\", F.unix_timestamp(\"tpep_dropoff_datetime\"))\n",
    "    .withColumn(\"trip_duration_min\", (F.col(\"dropoff_ts\") - F.col(\"pickup_ts\")) / 60.0)\n",
    "    .filter((F.col(\"trip_duration_min\") > 0) & (F.col(\"trip_duration_min\") < 300 ))#filter out outlier trip durations\n",
    "    \n",
    "    \n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"pick_lat\", radians(df_clean.pickup_latitude))\n",
    "    .withColumn(\"pick_long\", radians(df_clean.pickup_longitude))\n",
    "    .withColumn(\"drop_lat\", radians(df_clean.dropoff_latitude))\n",
    "    .withColumn(\"drop_long\", radians(df_clean.dropoff_longitude))\n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"lat_diff\", df_clean.pick_lat - df_clean.drop_lat)\n",
    "    .withColumn(\"long_diff\", df_clean.pick_long - df_clean.drop_long)\n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"a\", sin(df_clean.lat_diff/2)**2 + cos(df_clean.pick_lat)*cos(df_clean.drop_lat)*sin(df_clean.long_diff/2)**2)\n",
    "    .withColumn(\"c\", 2*atan2(sqrt(F.col(\"a\")), sqrt(1-F.col(\"a\"))))\n",
    ")\n",
    "df_clean=(\n",
    "    df_clean\n",
    "    .withColumn(\"straight_line_distance\", df_clean.c*R)\n",
    "\n",
    ")\n",
    "df_clean=df_clean.drop(\"pick_lat\", \"pick_long\", \"drop_lat\", \"drop_long\", \"lat_diff\", \"long_diff\", \"a\", \"c\")\n",
    "df_clean=df_clean.filter(F.col(\"straight_line_distance\")<125)#filter outlier trips\n",
    "df_clean=df_clean.withColumn(\"high_fare\", F.when(df_clean.fare_amount >20,1).otherwise(0))\n",
    "numericFeatures = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_duration_min\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dow\",\n",
    "    \"straight_line_distance\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#remove nulls for cols used as features + label\n",
    "colsToKeepNotNull = numericFeatures +  [\"fare_amount\"]\n",
    "\n",
    "df_clean = df_clean.na.drop(subset=colsToKeepNotNull)\n",
    "train_df, test_df = df_clean.randomSplit([0.7,0.3], seed=42)\n",
    "display(df_clean)\n",
    "df_clean.groupBy(\"high_fare\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126fe1af-a359-4378-9373-527034439c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#task 1 pipeline1\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/ml_storage/ml_schema/pipeline1\"\n",
    "\n",
    "cols = [\"pickup_hour\", \"pickup_dow\", \"trip_distance\", \"straight_line_distance\",\"trip_duration_min\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = cols,\n",
    "    outputCol = \"features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol = \"features\",\n",
    "    outputCol = \"scaled_features\"\n",
    ")\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    labelCol = \"high_fare\",\n",
    "    featuresCol = \"scaled_features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, decision_tree])\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "    .addGrid(decision_tree.maxDepth, [2, 5, 10])\n",
    "    .addGrid(decision_tree.minInstancesPerNode, [1, 2])\n",
    "    .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_fare\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "dt_model = cv.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "f1 = evaluator.evaluate(dt_predictions)\n",
    "precision_val = evaluator.setMetricName(\"weightedPrecision\").evaluate(dt_predictions)\n",
    "recall_val = evaluator.setMetricName(\"weightedRecall\").evaluate(dt_predictions)\n",
    "accuracy_val = evaluator.setMetricName(\"accuracy\").evaluate(dt_predictions)\n",
    "\n",
    "print(\"Decision Tree F1 Score:\", f1)\n",
    "print(\"Precision:\", precision_val)\n",
    "print(\"Recall:\", recall_val)\n",
    "print(\"Accuracy:\", accuracy_val)\n",
    "\n",
    "display(dt_predictions.select(\"fare_amount\", \"high_fare\", \"prediction\", \"pickup_hour\", \"pickup_dow\"))\n",
    "\n",
    "\n",
    "# Save the best model, might need to change depending on where youre saving it\n",
    "dt_model.bestModel.write().overwrite().save(\"/Volumes/ml_storage/ml_schema/ml_volume/models/dt_classifier_pipeline/\")\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1930c04-19c6-4f9f-b0b6-2ba2862ccbb7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764127008957}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/ml_storage/ml_schema/pipeline2\"\n",
    "\n",
    "cols = [\"pickup_hour\", \"pickup_dow\", \"trip_distance\", \"straight_line_distance\",\"trip_duration_min\"]\n",
    "\n",
    "assembler_lr = VectorAssembler(\n",
    "    inputCols = cols,\n",
    "    outputCol = \"features\"\n",
    ")\n",
    "\n",
    "scaler_lr = StandardScaler(\n",
    "    inputCol = \"features\",\n",
    "    outputCol = \"scaled_features\"\n",
    ")\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    labelCol = \"high_fare\",\n",
    "    featuresCol = \"scaled_features\"\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(stages = [assembler_lr, scaler_lr, log_reg])\n",
    "\n",
    "lr_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(log_reg.regParam, [0.01, 0.1])\n",
    "                 .addGrid(log_reg.maxIter, [20, 100])\n",
    "                 .build())\n",
    "\n",
    "lr_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"high_fare\",\n",
    "    predictionCol = \"prediction\",\n",
    "    metricName = \"f1\"\n",
    ")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator = lr_pipeline,\n",
    "    estimatorParamMaps = lr_param_grid,\n",
    "    evaluator = lr_evaluator,\n",
    "    numFolds = 3\n",
    ")\n",
    "\n",
    "lr_model = lr_cv.fit(train_df)\n",
    "lr_pred = lr_model.transform(test_df)\n",
    "\n",
    "f1_lr = lr_evaluator.evaluate(lr_pred)\n",
    "precision_lr = lr_evaluator.setMetricName(\"weightedPrecision\").evaluate(lr_pred)\n",
    "recall_lr = lr_evaluator.setMetricName(\"weightedRecall\").evaluate(lr_pred)\n",
    "accuracy_lr = lr_evaluator.setMetricName(\"accuracy\").evaluate(lr_pred)\n",
    "\n",
    "print(\"Logistic Regression F1 Score:\", f1_lr)\n",
    "print(\"Precision:\", precision_lr)\n",
    "print(\"Recall:\", recall_lr)\n",
    "print(\"Accuracy:\", accuracy_lr)\n",
    "\n",
    "display(lr_pred.select(\"fare_amount\", \"high_fare\", \"prediction\", \"pickup_hour\", \"pickup_dow\"))\n",
    "\n",
    "lr_model.bestModel.write().overwrite().save(\"/Volumes/ml_storage/ml_schema/ml_volume/models/lr_classifier_pipeline/\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9176f1ee-3904-44ef-be0d-a0cd835e82d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "accuracy_lr= 0.8864278378681718\n",
    "precision_lr= 0.7857565145535814\n",
    "recall_lr= 0.8864278378681718\n",
    "f1_lr= 0.833061794027467\n",
    "\n",
    "accuracy_val= 0.9903350688617122\n",
    "precision_val= 0.9903077071022511\n",
    "recall_val= 0.9903350688617122\n",
    "f1= 0.9903197664579257\n",
    "\n",
    "metrics=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "models = [\"Decision Tree\", \"Logistic Regression\"]\n",
    "lr_scores=[accuracy_lr, precision_lr, recall_lr, f1_lr]\n",
    "dt_scores=[accuracy_val, precision_val, recall_val, f1]\n",
    "\n",
    "position=np.arange(len(metrics))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(position-.2, dt_scores, 0.35, label=\"Decision Tree\")\n",
    "plt.bar(position +.2, lr_scores, 0.35, label=\"Logistic Regression\")\n",
    "\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Model Comparison: Decision Tree vs Logistic Regression\")\n",
    "plt.xticks(position, metrics)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31384d9-ae61-4e85-8731-bd1873415ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#2 pipeline 1\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "numericFeatures = [\n",
    "    \"trip_distance\",\n",
    "    \"straight_line_distance\",\n",
    "    \"trip_duration_min\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dow\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numericFeatures,\n",
    "    outputCol=\"features_unscaled\"\n",
    ")\n",
    "\n",
    "#scale for linear regression\n",
    "#withMean=False keeps vector sparse to avoid huge memory usage\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=False, #True\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "#linear regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "train_df, test_df = df_clean.randomSplit([0.7,0.3], seed=42)\n",
    "train_small = train_df.limit(100000) #due to memory limits we had to use a sample of 100k to be able to do the Cv\n",
    "\n",
    "#full linear regression pipeline\n",
    "lrPipeline = Pipeline(\n",
    "    stages=[assembler, scaler, lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d7c6d0-5c8d-46d9-bb9d-30207b95d9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for insane values in the features\n",
    "test_df.select(\n",
    "    F.max(\"trip_duration_min\"), \n",
    "    F.max(\"straight_line_distance\"), \n",
    "    F.max(\"trip_distance\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c0786d-750d-49a4-9842-4cb6ee2b5c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#param grid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.0, 0.0001, 0.001, 0.01])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5])\n",
    "    .addGrid(lr.maxIter, [50, 100])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluatorRmse = RegressionEvaluator(\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "cvLr = CrossValidator(\n",
    "    estimator=lrPipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluatorRmse,\n",
    "    numFolds=2,     #lower if needed\n",
    "    parallelism=1,  #lower if needed\n",
    "    collectSubModels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60e4bbc-e4b5-4204-bce8-86da2c004977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/ml_storage/ml_schema/ml_volume/sparkml_tmp\"\n",
    "cvLrModel = cvLr.fit(train_small)\n",
    "\n",
    "bestLrPipelineModel = cvLrModel.bestModel\n",
    "bestLrModel = bestLrPipelineModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4544b25-3299-4273-a370-7ea5a49bf84f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bestLrPipelineModel.write().overwrite().save(\n",
    "    \"/Volumes/ml_storage/ml_schema/ml_volume/models/linear_regression_pipeline/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6883307b-618c-4718-b71f-1620eb5f2c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions on test set\n",
    "testPredLr = bestLrPipelineModel.transform(test_df)\n",
    "\n",
    "#evaluation metrics\n",
    "rmseTest = evaluatorRmse.evaluate(testPredLr)\n",
    "evaluatorR2 = RegressionEvaluator(\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2Test = evaluatorR2.evaluate(testPredLr)\n",
    "\n",
    "print(\"Linear Regression Evaluation Metrics:\")\n",
    "print(\"RMSE:\", rmseTest)\n",
    "print(\"R2:\", r2Test)\n",
    "\n",
    "#predictions vs actual\n",
    "testPredLr.select(\"fare_amount\", \"prediction\").show(10)\n",
    "\n",
    "print(\"Best Linear Regression Hyperparameters\")\n",
    "print(\"regParam:\", bestLrModel.getRegParam())\n",
    "print(\"elasticNetParam:\", bestLrModel.getElasticNetParam())\n",
    "print(\"maxIter:\", bestLrModel.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25d78fa-fb26-43d1-a248-170a14f42777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSh0ZXN0UHJlZExyLnNlbGVjdCgiZmFyZV9hbW91bnQiLCAicHJlZGljdGlvbiIpKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewffaaac5\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewffaaac5\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewffaaac5\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewffaaac5) SELECT `fare_amount`,`prediction` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewffaaac5\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "fare_amount",
             "id": "column_d6d8004b159"
            },
            "y": [
             {
              "column": "prediction",
              "id": "column_d6d8004b160"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "hideXAxis": false,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_d6d8004b160": {
             "name": "prediction",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "cdc9fe5d-3228-4cb1-9f70-002c77bec5f6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.94140625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "fare_amount",
           "type": "column"
          },
          {
           "column": "prediction",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(testPredLr.select(\"fare_amount\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1b1451-1c93-4e06-8681-cab0359d6274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 pipeline 2\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rfPipeline = Pipeline(\n",
    "    stages=[assembler, scaler, rf]\n",
    ")\n",
    "\n",
    "rfparamGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20, 50])\n",
    "    .addGrid(rf.maxDepth, [5, 8])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "rfCV = CrossValidator(\n",
    "    estimator=rfPipeline,\n",
    "    estimatorParamMaps=rfparamGrid,\n",
    "    evaluator=evaluatorRmse, #same RMSE evaluator\n",
    "    numFolds=3,     #lower if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb1b2b7a-81c0-4bac-a5a3-8fdd0997195d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rfCvModel = rfCV.fit(train_small)\n",
    "\n",
    "bestRfPipelineModel = rfCvModel.bestModel\n",
    "bestRfModel = bestRfPipelineModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95353329-dff9-4d6c-a7c5-bcd4ce111aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions on test set\n",
    "testPredRf = bestRfPipelineModel.transform(test_df)\n",
    "\n",
    "#evaluation metrics\n",
    "rmseTest = evaluatorRmse.evaluate(testPredRf)\n",
    "evaluatorR2 = RegressionEvaluator(\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2Test = evaluatorR2.evaluate(testPredRf)\n",
    "\n",
    "print(\"Random Forest Regressor Pipeline Evaluation Metrics:\")\n",
    "print(\"RMSE:\", rmseTest)\n",
    "print(\"R2:\", r2Test)\n",
    "\n",
    "#predictions vs actual\n",
    "testPredRf.select(\"fare_amount\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec9516f-3cad-4170-8a52-76d951c1d0a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSh0ZXN0UHJlZFJmLnNlbGVjdCgiZmFyZV9hbW91bnQiLCAicHJlZGljdGlvbiIpKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView8b837f2\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView8b837f2\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView8b837f2\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView8b837f2) SELECT `fare_amount`,`prediction` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView8b837f2\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "fare_amount",
             "id": "column_d6d8004b153"
            },
            "y": [
             {
              "column": "prediction",
              "id": "column_d6d8004b154"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_d6d8004b151": {
             "name": "prediction",
             "type": "scatter",
             "yAxis": 0
            },
            "prediction": {
             "type": "scatter",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "18ac6a8f-c442-4e21-87b3-fc6b720ecee2",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.95166015625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "fare_amount",
           "type": "column"
          },
          {
           "column": "prediction",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(testPredRf.select(\"fare_amount\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24031887-e1f5-422e-8d89-3a6d5e41cd76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best Random Forest Hyperparameters\")\n",
    "print(\"numTrees:\", bestRfModel.getNumTrees)\n",
    "print(\"maxDepth:\", bestRfModel.getMaxDepth())\n",
    "\n",
    "#save the best model, might need to change depending on where youre saving it\n",
    "bestRfPipelineModel.write().overwrite().save(\"/Volumes/ml_storage/ml_schema/ml_volume/models/RfR_pipeline\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee01140b-69f5-4f76-ac76-5ca42161bff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "#how to reload models\n",
    "logistic_reg = PipelineModel.load(\"/Volumes/ml_storage/ml_schema/ml_volume/models/lr_classifier_pipeline\")\n",
    "rfr_load = PipelineModel.load(\"/Volumes/ml_storage/ml_schema/ml_volume/models/RfR_pipeline\")\n",
    "dt_tree = PipelineModel.load(\"/Volumes/ml_storage/ml_schema/ml_volume/models/dt_classifier_pipeline\")\n",
    "linear_reg = PipelineModel.load(\"/Volumes/ml_storage/ml_schema/ml_volume/models/linear_regression_pipeline\")\n",
    "\n",
    "#example\n",
    "\n",
    "logistic_reg.transform(test_df).select(\"high_fare\",\"prediction\").show(10)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "keenan's notebook A3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
